import arxiv
import requests
import os
import time
import json
import re

# === 1. é…ç½®åŒºåŸŸ ===
KEYWORDS = [
    '"Humanoid Robot" AND "Reinforcement Learning"',
    '"Embodied AI" AND "Transformer"',
    '"Vision-Language-Action"',
    '"Sim-to-Real" AND "Humanoid"',
    '"Robot Manipulation" AND "Foundation Model"'
]

SAVE_DIR = r"/Users/xiuliansu/Documents/å¤§å››ä¸Šå­¦æœŸ/01 å­¦ä¸šå¸ƒå±€å’ŒèŒä¸šè§„åˆ’/0104 è¡Œä¸šç ”æŠ¥ä¸è®ºæ–‡/Humanoid_Brain_Papers"
HISTORY_FILE = os.path.join(SAVE_DIR, "download_history.json")

# === ä»£ç†è®¾ç½® (å¦‚æœä½ åœ¨ç”¨ VPNï¼Œè¯·ç¡®ä¿è¿™é‡Œè®¾ç½®æ­£ç¡®ï¼Œå¦åˆ™æ³¨é‡Šæ‰) ===
# os.environ['http_proxy'] = "http://127.0.0.1:7890"
# os.environ['https_proxy'] = "http://127.0.0.1:7890"

if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# === 2. æ ¸å¿ƒå·¥å…·å‡½æ•° ===
def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            try: return set(json.load(f))
            except: return set()
    return set()

def save_history(history_set):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(list(history_set), f, indent=2)

def get_chinese_blog_search_links(title):
    return {
        "æœºå™¨ä¹‹å¿ƒ": f"https://www.jiqizhixin.com/search?q={title}",
        "æ–°æ™ºå…ƒ": f"https://weixin.sogou.com/weixin?type=2&query=æ–°æ™ºå…ƒ {title}"
    }

# === [æ ¸å¿ƒä¿®å¤] è‡ªå®šä¹‰å¼ºåŠ›ä¸‹è½½å‡½æ•° ===
def download_file_robust(url, filepath, retries=3):
    """æ›¿ä»£åº“è‡ªå¸¦çš„ä¸‹è½½ï¼Œä½¿ç”¨ requests åº“ï¼Œæ›´ç¨³å®š"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    # æ—¢ç„¶å·²ç»æ‹¿åˆ° url (é€šå¸¸æ˜¯ /abs/), æˆ‘ä»¬è¦è½¬æ¢æˆ PDF é“¾æ¥
    # arxiv åº“ç»™çš„ pdf_url å±æ€§é€šå¸¸æ˜¯ http://arxiv.org/pdf/xxxx.xxxxxv1
    if "abs" in url:
        url = url.replace("abs", "pdf")
    
    # ç¡®ä¿ URL ä»¥ .pdf ç»“å°¾ (è™½ç„¶ ArXiv æœ‰æ—¶ä¸éœ€è¦ï¼Œä½†åŠ ä¸Šæ›´ä¿é™©)
    if not url.endswith(".pdf"):
        url += ".pdf"

    for i in range(retries):
        try:
            # timeout=60ç§’ï¼Œstream=True å…è®¸å¤§æ–‡ä»¶
            response = requests.get(url, headers=headers, stream=True, timeout=60)
            response.raise_for_status() # æ£€æŸ¥ 404/403/429
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            return True # ä¸‹è½½æˆåŠŸ
            
        except Exception as e:
            print(f"      âš ï¸ ä¸‹è½½ä¸­æ–­ (å°è¯• {i+1}/{retries}): {e}")
            if "429" in str(e):
                print("      ğŸ›‘ è§¦å‘429é™æµï¼Œå¼ºåˆ¶ä¼‘æ¯ 60 ç§’...")
                time.sleep(60)
            else:
                time.sleep(5)
            
            # å¦‚æœæ˜¯æœ€åä¸€æ¬¡é‡è¯•è¿˜å¤±è´¥ï¼Œåˆ é™¤å¯èƒ½æŸåçš„æ–‡ä»¶
            if i == retries - 1 and os.path.exists(filepath):
                os.remove(filepath)
                
    return False

# === 3. ä¸»é€»è¾‘ ===
def scrape_arxiv_papers(max_results=10):
    downloaded_ids = load_history()
    client = arxiv.Client(page_size=10, delay_seconds=3.0, num_retries=3)
    
    print(f"ğŸš€ å¼€å§‹æœç´¢ (å¼ºåŠ›ä¸‹è½½æ¨¡å¼)...")
    
    for query in KEYWORDS:
        print(f"\nğŸ” æœç´¢: {query}")
        try:
            search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
            
            # ä½¿ç”¨åˆ—è¡¨è½¬æ¢ï¼Œé˜²æ­¢ç”Ÿæˆå™¨è¶…æ—¶
            results = list(client.results(search))
            
            for result in results:
                try:
                    if result.published.year < 2023: continue
                    
                    paper_id = result.entry_id.split('/')[-1]
                    if paper_id in downloaded_ids: continue

                    safe_title = "".join([c for c in result.title if c.isalnum() or c in " ._-"]).strip()[:150]
                    pdf_path = os.path.join(SAVE_DIR, f"{safe_title}.pdf")
                    info_path = os.path.join(SAVE_DIR, f"{safe_title}_info.txt")

                    if os.path.exists(pdf_path):
                        downloaded_ids.add(paper_id)
                        continue

                    print(f"   â¬‡ï¸ ä¸‹è½½: {safe_title[:40]}...")
                    
                    # === ä½¿ç”¨è‡ªå®šä¹‰ä¸‹è½½å‡½æ•° ===
                    # result.pdf_url æ˜¯ ArXiv åº“æä¾›çš„ PDF é“¾æ¥
                    success = download_file_robust(result.pdf_url, pdf_path)
                    
                    if success:
                        print(f"      âœ… å®Œæˆ")
                        blog_links = get_chinese_blog_search_links(result.title)
                        info_content = f"Title: {result.title}\nID: {paper_id}\nSummary:\n{result.summary}\n\næœºå™¨ä¹‹å¿ƒ: {blog_links['æœºå™¨ä¹‹å¿ƒ']}\næ–°æ™ºå…ƒ: {blog_links['æ–°æ™ºå…ƒ']}"
                        with open(info_path, "w", encoding="utf-8") as f: f.write(info_content)
                        
                        downloaded_ids.add(paper_id)
                        save_history(downloaded_ids)
                        time.sleep(5) # ä¹–ä¹–ä¼‘æ¯
                    else:
                        print(f"      âŒ ä¸‹è½½æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡ã€‚")

                except Exception as e:
                    print(f"   âš ï¸ å¤„ç†å•ç¯‡å‡ºé”™: {e}")

        except Exception as e:
             print(f"âš ï¸ æœç´¢å‡ºé”™ (å¯èƒ½æ˜¯429): {e}")
             time.sleep(20)

    print(f"\nâœ¨ ä»»åŠ¡ç»“æŸ")

if __name__ == "__main__":
    scrape_arxiv_papers(max_results=10)