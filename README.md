# Arvix_technical_-papers
检索具身智能领域的论文

# 🤖 ArXiv 具身智能论文强力捕获器 (Embodied AI Paper Scraper)

一个兼顾稳定性与业务洞察的自动化 Python 脚本，专门用于定向追踪、强力下载并管理“人形机器人”与“具身智能”领域的前沿 ArXiv 论文。

## 💡 开发背景

在追踪快速迭代的硬科技赛道（如 Humanoid、Vision-Language-Action 等）时，频繁手动检索论文不仅耗时，还容易遗漏核心进展。同时，ArXiv 原生 API 或第三方库在批量下载时，极易因网络波动或触发反爬封锁（429 Too Many Requests）而导致任务中断。

为了更高效地构建个人的前沿技术知识库，以便快速提炼技术路线、支撑深度的行业分析与商业化研报，本脚本重构了底层的下载逻辑，并加入了增量追踪与周边研报辅助功能。

## ⚙️ 核心运行机制 (四步走流水线)

本脚本彻底摒弃了脆弱的默认下载器，构建了一条“检索-过滤-下载-建档”的自动化流水线：

1. **定向巡航检索**
   - 遍历预设的高价值关键词矩阵（如 `"Sim-to-Real" AND "Humanoid"`）。
   - 严格拦截陈旧数据，**仅放行 2023 年及以后的最新论文**。

2. **本地去重校验**
   - 读取本地 `download_history.json` 历史库，跳过已下载文献，实现优雅的**断点续传**与**增量更新**。

3. **自定义强力下载 (`download_file_robust`)**
   - **伪装与转换**：自动注入 `User-Agent`，并将 `/abs/` 链接精准切流至 `/pdf/` 下载源。
   - **分块抗压**：采用 `requests` 流式下载（`stream=True`），配合 60 秒超时控制。
   - **智能避险**：内置 3 次阶梯式重试机制；一旦捕获 `429` 封锁状态码，立刻强制休眠 60 秒，保护本地 IP 资产。

4. **双轨建档与情报延展**
   - **本体归档**：将 PDF 保存为清洗过特殊字符的规范文件名。
   - **情报延展**：同步生成 `_info.txt` 摘要文件，不仅包含论文 Abstract，还自动拼装**机器之心**与**新智元**的中文深度解读检索链接，大幅缩短从“读论文”到“懂商业逻辑”的转化路径。

## ✨ 功能亮点

- **🛡️ 极致稳定**：再也不用担心半夜跑数据时因为网络抖动而前功尽弃，失败自动清理残缺文件。
- **📚 知识库隔离**：自动在指定路径创建专属文件夹，PDF 与 TXT 摘要成对存放，专治文件杂乱。
- **🔗 投研视角辅助**：一键直达国内头部 AI 科技媒体的解读文章，辅助评估该技术的商业化成熟度与市场热度。

## 🛠️ 安装与运行

1. 安装必要的依赖库：
```Bash
pip install arxiv requests
```

2.（可选）如果你在特定的网络环境下，请在脚本中解除代理配置的注释：
   ```Python
   os.environ['http_proxy'] = "[http://127.0.0.1:7890](http://127.0.0.1:7890)"
   ```

3.运行脚本：
   ```Bash
   python arxiv_scraper.py
   ```

## 🔧 自定义配置
你可以在代码的 # === 1. 配置区域 === 直接修改核心参数：
 - KEYWORDS: 根据近期的研究重心随时更换追踪词（如脑机接口、端到端自动驾驶等）。
 - SAVE_DIR: 定义你的本地研报与论文知识库存储路径。
 - max_results: 控制每个关键词下抓取的最大论文数量（默认 10 篇）。